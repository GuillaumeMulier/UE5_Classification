---
title:  "Devoir Classification et Apprentissage session 1"
author: "Benoit Gachet, Guillaume Mulier"
date: "`r format(Sys.Date(), '%d/%m/%Y')`"
output: 
  word_document:
    reference_docx: template_word.docx
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      cache = TRUE)
```

```{r packages, include = FALSE, cache = FALSE}
library(tidyverse)
library(patchwork)
library(ggtext)
library(FactoMineR)
library(factoextra)
library(dendextend)
library(purrr)
library(scales)
library(mclust)
library(cowplot)
library(ggpubr)
library(FNN)
library(randomForest)
library(flextable)
```

```{r helpers, include = FALSE, cache = FALSE}
ggplot2::theme_set(theme_light() +
                     theme(plot.title = element_markdown(size = 10),
                           plot.subtitle = element_markdown(size = 7),
                           strip.background = element_blank(),
                           strip.text = element_textbox(
                             size = 12, 
                             color = "white", fill = "#7888C0", box.color = "#000066",
                             halign = 0.5, linetype = 1, r = unit(3, "pt"), width = unit(0.75, "npc"),
                             padding = ggplot2::margin(2, 0, 1, 0), margin = ggplot2::margin(3, 3, 3, 3))))
```

# Exercice classification non supervisée

```{r chargement-données, include = FALSE}
les_inconnus_reduits <- read_delim("DonneesNSGpe.txt", delim = "\t") %>% 
  filter(Groupe == 7) %>% 
  select(-Groupe) %>% 
  mutate(across(everything(), .fns = ~ (.x - mean(.x)) / sd(.x)))
les_inconnus_shopenhauer <- les_inconnus_reduits
```

## Introduction

L’objectif de notre devoir était de réaliser une classification non supervisée afin de caractériser l’appartenance des observations de notre base de données à k classes, en définissant le nombre de classes.

Notre base de données comportait un ensemble de 500 observations et 10 variables mesurées pour chacune des observations. Nous n’avions aucune donnée manquante pour l'ensemble de la base. Les ordres de grandeur des variables étaient similaires ; néanmoins, afin d’éviter qu’une variable ne prenne trop d’importance du simple fait de son unité de mesure, nous avons centré et réduit l’ensemble des variables.

Il existe de nombreuses méthodes de classification non supervisées. Nous nous sommes intéressés à 3 méthodes : la classification hiérarchique ascendante, les k-means et les modèles de mélange Gaussiens. Nous les avons ensuite comparé et avons discuté les avantages et les inconvénients de chacune dans notre classification.

## Mise en oeuvre des modèles

### Description du jeu de données

Pour visualiser les données, nous avons mis en oeuvre une analyse en composantes principales. Les 2 1^ères^ composantes principales expliquent la quasi-totalité de l'inertie du jeu de données. Nous utiliserons donc exclusivement ces 2 dimensions pour explorer le jeu de données.

En représentant les projections des individus sur ces 2 composantes, nous pouvons remarquer un nuage de points bien étallé. Et on a l'impression de distinguer 3 groupes de points dans ce nuage. Ces 3 groupes ont l'air d'être de forme sphérique, ce qui guidera nos choix de classifications par la suite.

```{r acp, fig.width = 9, fig.height = 7}
ACP_inc <- PCA(les_inconnus_reduits, scale.unit = TRUE, graph = FALSE)
plot_axes <- fviz_pca_var(ACP_inc, repel = TRUE) +
  labs(title = "Cercle des corrélations des variables aux axes 1 et 2") +
  theme(plot.title = element_markdown(size = 10))
plot_ind <- fviz_pca_ind(ACP_inc, geom.ind = "point") +
  labs(title = "Projection des individus sur les 2 1<sup>ers</sup> axes de l'ACP") +
  theme(plot.title = element_markdown(size = 10))
plot_ind + plot_axes
```

### Classification ascendante hiérarchique

Nous avons dans un premier temps réalisé une classification hiérarchique ascendante et présenté ses résultats sous la forme d'un dendrogramme. 

Etant donné le résultat de l'ACP avec la projection des variables sur un plan qui explique la quasi-totalité de l'inertie, ainsi que le caractère numérique de toutes nos variables prédictrices, nous pouvons bien nous mettre dans la situation d'un espace métrique. De ce fait, nous avons choisi d'utiliser la distance euclidienne entre les différents individus statistiques, et la méthode pour aggréger les individus est la méthode de Ward, adaptée aux clusters sphériques.

Pour déterminer le nombre de clusters, nous avons calculé l'inertie intra-classe en fonction du nombre de clusters. Puis, notre nombre de clusters était le nombre pour lequel il y a une rupture dans la diminution d'inertie intra-classe (stratégie, parfois dénommée ”critère du coude”). Cette classification permet de créer 3 classes d’effectifs semblables puisqu’ils sont respectivement de 156, 168, 176.

```{r cah-dend, message = FALSE, warning = FALSE, fig.width = 10, fig.height = 6}
set.seed(7)
dendro <- les_inconnus_reduits %>% 
  dist(method = "euclidean") %>% 
  hclust(method = "ward.D2") 
dendro_marches <- cbind(x = 1:500, inertie = sort(dendro$height, decreasing = TRUE), color = c(1, 1, 1, rep(2, 497))) %>% 
  as.data.frame() %>% 
  slice(1:20) %>% 
  ggplot(aes(x = x, y = inertie)) +
  geom_step() +
  geom_point(aes(color = as.factor(color)), show.legend = FALSE) +
  geom_vline(xintercept = 3.5, color = "blue") +
  scale_color_manual(values = c("blue", "black")) +
  scale_x_continuous(breaks = 1:20) +
  labs(x = "Nombre de branches",
       y = "Inertie intra-classe",
       title = "Décroissance de l'inertie avec le nombre de branches") +
  theme(panel.grid.minor.x = element_blank())
dendro_arbre <- dendro %>% 
  as.dendrogram() %>%
  set("branches_k_color", k = 3) %>% 
  set("branches_lwd", 0.5) %>%
  set("labels_cex", 0) %>%
  set("leaves_pch", 10) %>% 
  set("leaves_cex", 0.1) %>% 
  as.ggdend() %>% 
  ggplot(theme = theme_light()) +
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        panel.grid.minor.x = element_blank()) +
  labs(y = "Inertie intra-classe",
       x = NULL,
       title = "Classification ascendante hiérarchique selon la distance euclidienne") +
  geom_hline(yintercept = 25, linetype = "dashed", color = "purple3") +
  geom_segment(aes(x = 168, xend = 168, y = 25, yend = 0), linetype = "dashed", color = "purple3") +
  geom_segment(aes(x = 324, xend = 324, y = 25, yend = 0), linetype = "dashed", color = "purple3")
dendro_marches + dendro_arbre
les_inconnus_shopenhauer$class_cah <- cutree(dendro, k = 3)
```
En réprésentant les individus sur les 2 1^ères^ composantes principales, nous pouvons voir les 3 clusters qui sont dans 3 directions du plan représenté.

```{r cah-ind, fig.width = 9, fig.height = 7}
ACP_inc <- PCA(les_inconnus_shopenhauer, scale.unit = TRUE, graph = FALSE, quali.sup = 11)
plot_ind2 <- fviz_pca_ind(ACP_inc, geom.ind = "point", habillage = 11) +
  labs(title = "Projection des individus sur les 2 1<sup>ers</sup> axes de l'ACP",
       subtitle = "Colorés selon les groupes faits par CAH") +
  theme(plot.title = element_markdown(size = 10),
        plot.subtitle = element_text(size = 7),
        legend.position = "none")
plot_ind2 + plot_axes
```

### Les K-means

```{r fct-kmeans, include = FALSE}
replication_kmeans <- function(data, n_rep, nb_clust, result = "modele", nstart) {
  result <- match.arg(result, c("modele", "liste"))
  kmeans_rep <- replicate(n_rep, kmeans(data, centers = nb_clust, nstart = nstart))
  best <- which.min(unlist(kmeans_rep['tot.withinss', ]))
  kmeans <- kmeans_rep[, best]
  if (result == "modele") {
    return(kmeans)
  } else if (result == "liste") {
    return(kmeans_rep)
  }
}
```

Pour sélectionner le nombre de groupes optimal par les méthodes des K-means, nous avons eu recours à 2 méthodes. La 1^ère^ est analogue à celle utilisée pour la classification ascendante hiérarchique : nous avons repéré le saut dans la diminution de l'inertie intra-classe avec le nombre de classes. Nous avons aussi représenté l'indice de Calinski-Harabasz qui doit être maximisé. Ces 2 méthodes nous indiquent un nombre optimal de clusters de 3. Nous réaliserons donc des clusters selon la méthode des K-means avec k = 3.

```{r choix-k, fig.width = 10, fig.height = 6}
set.seed(7)
modeles <- vector(mode = "list", length = 10)
for (i in 1:10) {
  modeles[[i]] <- replication_kmeans(data = les_inconnus_reduits,
                                     nb_clust = i, n_rep = 25, nstart = 25)
}
plot_inertie <- map_dfr(modeles, ~ c(intra = .x[["tot.withinss"]]), .id = "k") %>% 
  bind_cols(color = c(1, 1, 1, rep(2, 7))) %>% 
  ggplot(aes(x = as.numeric(k), y = intra)) +
  geom_point(aes(color = as.factor(color)), show.legend = FALSE) +
  geom_step() +
  geom_vline(xintercept = 3.5, color = "blue") +
  scale_color_manual(values = c("blue", "black")) +
  scale_x_continuous(breaks = 1:10) +
  labs(x = "Nombre de clusters",
       y = "Inertie intra-clusters",
       title = "Décroissance de l'inertie avec le nombre de clusters",
       subtitle = "Sélection de la meilleure classification parmi 25 réplicats à chaque k") +
  theme(panel.grid.minor.x = element_blank())
# Là encore, il y a un saut dans les inerties pour k = 3 environ
plot_calinski <- map_dfr(modeles, ~ c(intra = .x[["tot.withinss"]], inter = .x[["betweenss"]]), .id = "k") %>% 
  mutate(k = as.numeric(k),
         calinski = ifelse(k == 1, 0, ((500 - k) * inter) / ((k - 1) * intra)),
         color = c(2, 2, 1, rep(2, 7))) %>% 
  ggplot(aes(x = k, y = calinski)) +
  geom_point(aes(color = as.factor(color)), show.legend = FALSE) +
  geom_line() +
  geom_vline(xintercept = 3, color = "blue", linetype = "dashed") +
  scale_color_manual(values = c("blue", "black")) +
  scale_x_continuous(breaks = 1:10) +
  labs(x = "Nombre de clusters",
       y = "Indice de Calinski-Harabasz",
       title = "Evolution de l'indice de Calinski-Harabasz en fonction du nombre\nde clusters faits") +
  theme(panel.grid.minor.x = element_blank())
# k = 3 maximise l'indice de calinski
plot_inertie + plot_calinski
```

Pour valider la stabilité de la méthode à l'initialisation aléatoire, nous avons généré 25 réplicats d'une sélection de 3 groupes par les K-means. Les 3 clusters créés ont des effectifs similaires autour de 160-170 dans 24 des réplicats. Mais le 2^ème^ réplicat montre un cluster qui a plus de patients que les 2 autres.

Nous avons donc regardé si les individus étaient répartis différemment selon le réplicat. Pour ce faire, pour les réplicats 2 à 25, nous avons attribué le numéro de cluster du réplicat 1 pour lequel l'effectif dans le réplicat (2 à 25) est majoritaire. Nous avons constaté que dans 70% des individus statistiques, le cluster est constant avec les 25 réplicats. En revanche, pour 30% des individus, on a des clusters différents avec les réplicats. En analysant les séquences de numéros de clusters, on a un déséquilibre pour le réplicat n°2. On peut remarquer que dans ce réplicat, en aggrégeant avec notre méthode, le cluster 1 du réplicat disparaît car il est minoritaire dans tous les clusters du réplicat 1. Ainsi, notre méthode n'est pas totalement satisfaisante dans ce cas de figure précis. Néanmoins, elle nous a permis de montrer la non constance de la classification pour le 2^ème^ réplicat. De plus, nous avons représenté la matrice de similarité des patients qui réprésente les mêmes patients en abscisse et en ordonnée, et les intersections entre 2 individus sont de plus en plus bleues en fonction du nombre de fois où ces 2 individus sont dans le même cluster parmi chaque réplicat. On voit là encore qu'il y a un léger déséquilibre avec un cluster qui n'est pas homogène avec les clusters des autres réplicats et qui est vraisemblablement le cas du réplicat 2.

```{r kmeans-25, fig.width = 10, fig.height = 6}
kmeans_replicats <- replication_kmeans(data = les_inconnus_reduits, nb_clust = 3, n_rep = 25, result = "liste", nstart = 1)
plot_replicats <- kmeans_replicats[7, ] %>% 
  map_dfr(~ c(c1 = .x[1], c2 = .x[2], c3 = .x[3])) %>% 
  bind_cols(rep = 1:25) %>% 
  pivot_longer(cols = -rep) %>% 
  ggplot(aes(rep, value, color = name)) +
  geom_point() +
  geom_line() +
  scale_color_discrete(name = "Clusters :", labels = c("1", "2", "3")) +
  coord_cartesian(ylim = c(80, 320)) +
  labs(x = "Numéro de réplication",
       y = "Effectif",
       title = "Vérification de la taille des clusters en réplicant 25 fois les K-means")
n <- 500
mat <-  matrix(0, nrow = n, ncol = n) # matrice carrée de taille n de 0
clusters <- kmeans_replicats[1, ] %>% 
  map_dfr(~ c(ind = .x)) %>% 
  t() %>% 
  as.data.frame()
clusters <- clusters[order(clusters$V1), ]
# Tri du tableau selon le 1er vecteur de clusters
for (r in seq_len(ncol(clusters))) {
  classe <- clusters[, r]
  for (i in 1:(n-1)) {
    for(j in (i + 1):n) {
      if (classe[i] == classe[j]) mat[i, j] <- mat[i, j] + 1
    }
  }
}
plot_similar <- (mat / ncol(clusters)) %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  pivot_longer(-rowname) %>% 
  mutate(rowname = as.numeric(rowname),
         name = as.numeric(str_remove(name, "^V"))) %>% 
  ggplot(aes(rowname, name, fill = value)) +
  geom_tile() +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_gradient2(low = "white", mid = "green", high = "blue", 
                       midpoint = 0.5, labels = percent_format(),
                       name = NULL) +
  labs(x = NULL,
       y = NULL) +
  annotate(geom = "richtext", x = 100, y = 100, size = 3, hjust = 0, vjust = 1,
           label = "**Visualisation des individus triés selon la <br> 1<sup>ère</sup> réplication de K-means :<br>Pourcentage de fois où les 2 individus sont <br>dans le même cluster sur les 25 réplicats**")
plot_replicats + plot_similar
```

Ainsi, la méthode des K-means apparaît robuste pour classifier notre jeu de données puisque les clusters sont presque constants tout au long des 25 réplicats.

Pour sélectionner notre classification finale, nous avons réalisé là encore 25 réplicats de la classification, puis nous avons sélectionné la classification qui minimisait le plus l'inertie intra-clusters. Nous avons représenté cette classification sur la figure suivante.

```{r plot-kmeanacp, fig.width = 9, fig.height = 7}
kmeans_3 <- replication_kmeans(data = les_inconnus_reduits, nb_clust = 3, n_rep = 25, result = "mode", nstart = 25)
kmeans_3$cluster <- factor(kmeans_3$cluster,
                           levels = c(1, 2, 3), labels = c(2, 1, 3)) %>% 
  as.character() %>% as.numeric()
class(kmeans_3) <- "kmeans"
plot_ind3 <- fviz_cluster(kmeans_3, les_inconnus_reduits, ellipse.type = "convex",
                          geom = "point", ggtheme = theme_light()) +
  labs(title = "Projection des individus sur les 2 1<sup>ers</sup> axes de l'ACP",
       subtitle = "Colorés selon les groupes faits par méthode K-means") +
  theme(plot.title = element_markdown(size = 10),
        plot.subtitle = element_text(size = 7),
        legend.position = "none") +
  scale_x_reverse(labels = function(x) -x)
plot_ind3 + plot_axes 
les_inconnus_shopenhauer$class_kmeans <- kmeans_3$cluster
```

### Modèle de mélanges Gaussiens

A la vue des résultats de l'ACP, nous nous sommes concentré sur les modèles sphériques (EII et VII). Nous avons écarté la méthode EII de notre analyse principale car c'était redondant avec les K-means.

Afin de choisir le nombre de classes, nous nous sommes appuyés sur l’analyse des maximums de vraissemblance, représentés par le score BIC et ICL. Néanmoins, la représentation suivante montre que le BIC et l'ICL n'atteignent pas de valeur maximale pour un nombre de classes de 20 ou moins. Etant donné nos résultats précédents et la représentation de l'ACP, nous nous sommes orienté vers 3 classes. De plus ça a l'air d'être le moment ou il y a une inflexion de l'augmentation du BIC/ICL. Nous avons regardé ce qui se passait pour les modèles non sphériques de matrice de covariance. Pour les modèles non orientés sur les axes, la fonction `Mclust` n'arrivait pas à calculer de classes (dans l'article de Scrucca, il est dit que cela demande une correction de la matrice de covariance), et pour les autres, on n'atteignait pas de maximum ni pour le BIC ni pour l'ICL.

```{r choix-kgaus, fig.width = 10, fig.height = 6}
set.seed(7)
boules_gaus <- purrr::map(1:20, ~ Mclust(les_inconnus_reduits, G = .x, modelNames = "VII"))
boules_gaus %>% 
  map_dfr(~ c(BIC = as.numeric(.x[["bic"]]), ICL = as.numeric(.x[["icl"]])), .id = "k") %>% 
  mutate(k = as.numeric(k)) %>% 
  pivot_longer(-k) %>% 
  ggplot(aes(x = k, y = value)) +
    geom_point() +
    geom_line() +
    labs(x = "Nombre de clusters",
         y = NULL,
         title = "Evolution du BIC et de l'ICL en fonction du nombre de clusters formés en méthode EII") +
    facet_wrap(vars(name), nrow = 1)
```

Nous avons représenté notre classification à 3 classes sur les 2 1^ers^ axes principaux et pouvont constater que les groupes formés sont similaires à ceux obtenus avec les méthodes précédentes avec les ellipses de concentrations selon la loi normale.

```{r fig-gaus3, fig.width = 9, fig.height = 7}
boules_gaus3 <- boules_gaus[[3]]
plot_ind_gaus3 <- fviz_cluster(boules_gaus3, les_inconnus_reduits, ellipse.type = "norm",
                               geom = "point", ggtheme = theme_light()) +
  labs(title = "Projection des individus sur les 2 1<sup>ers</sup> axes de l'ACP",
       subtitle = "Colorés selon les groupes faits par méthode des mélanges Gaussiens",
       caption = "Ellipse de concentration selon la loi normale") +
  theme(plot.title = element_markdown(size = 10),
        plot.subtitle = element_text(size = 7),
        legend.position = "none")
plot_ind_gaus3 <- plot_ind_gaus3 +
  scale_x_reverse(labels = function(x) -x)
plot_ind_gaus3 + plot_axes 
les_inconnus_shopenhauer$class_gaus <- boules_gaus3$classification
```

## Discussion et comparaisons des modèles

L’objectif étant in fine la mise en évidence de la structure sous-jacente des données, une méthode pragmatique pour le choix du nombre de classes est de choisir une partition dont il sera possible d’interpréter les classes.

Le modèle Gaussien minimise la variabilité intra-groupe, mais la sélection du nombre de groupes n'a pas l'air de s'arrêter (sur les critères du BIC et de l'ICL). Nous avons représenté des nombres de clusters croissants sur la figure suivante. En augmentant le nombre de clusters, on remarque que cela ne fait que segmenter les 3 groupes que nous avions définis. Il y a donc peut-être des groupes d'individus à l'intérieur de nos 3 clusters qui se ressemblent et peuvent se regrouper, mais le découpage en 3 groupes apparaît pertinent. La méthode des mélanges Gaussiens n'est peut-être ici pas adaptée pour déterminer le nombre de groupes.

```{r clust-gaus, fig.height = 9, fig.width = 9}
plot_ind_gaussian <- purrr::map(.x = c(3, 5, 7, 9),
                                .f = ~ fviz_cluster(boules_gaus[[.x]], les_inconnus_reduits, ellipse.type = "norm",
                                                    geom = "point", ggtheme = theme_light()) +
                                  labs(title = NULL,
                                       subtitle = paste0("Colorés selon les groupes faits par méthode des mélanges Gaussiens\nk = ", .x)) +
                                  theme(plot.title = element_markdown(size = 10),
                                        plot.subtitle = element_text(size = 7),
                                        legend.position = "none") +
                                  scale_x_reverse(labels = function(x) -x))
wrap_plots(plot_ind_gaussian) + plot_annotation(title = "Modèles de mélanges Gaussiens pour K croissants")
```

Les groupes formés par les 3 méthodes sont très similaires, et on voit que peu d'individus sont dans des clusters différents entre les méthodes. La différence se fait surtout sur les individus à la frontière entre les groupes 1 et 3 (aux coordonnées x = 1 et y = -0.5 environ sur les 2 1^ers^ axes principaux). Nous pensons donc avoir une classification assez proche de la réalité. La méthode des mélanges Gaussiens donne les 3 mêmes groupes environ, mais a plus de mal pour déterminer le nombre de groupes.

```{r comp-nonsup, fig.width = 9, fig.height = 9}
plot_diff <- les_inconnus_shopenhauer %>%
  select(starts_with("class")) %>%
  arrange(class_cah) %>%
  rownames_to_column() %>%
  mutate(rowname = as.numeric(rowname)) %>%
  pivot_longer(-rowname) %>%
  mutate(name = fct_recode(name,
                           "CAH" = "class_cah",
                           "Mélanges de\nGaussiennes" = "class_gaus",
                           "K-means" = "class_kmeans")) %>%
  ggplot(aes(x = as.factor(name), y = rowname, fill = as.factor(value))) +
  scale_x_discrete(name = "Techniques de classification non supervisée", expand = c(0, 0)) +
  scale_y_continuous(name = "Nombre d'individus", expand = c(0, 0)) +
  scale_fill_discrete(name = "Cluster n°") +
  geom_tile() +
  purrr::map(c(1.5, 2.5), ~ geom_vline(aes(xintercept = .x), size = 1, color = "black")) +
  labs(title = "Clusters d'appartenance pour chaque technique")
(plot_ind2 + plot_ind3) / (plot_ind_gaus3 + plot_diff)
```

# Exercice classification supervisée

## Introduction

L’objectif de notre devoir était de réaliser une classification supervisée en utilisant les données de la base simulée ElecGrid qui s’intéresse à la stabilité du réseau électrique. Nous devions réaliser plusieurs méthodes de classification, valider nos résultats et comparer les différentes méthodes entre elles.

```{r import-elec, include = FALSE, cache = FALSE}
dix_pour_cent_reduits <- read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv") %>% 
  select(-p1, -stab) %>% 
  mutate(across(tau1:g4, .fns = ~ (.x - mean(.x)) / sd(.x)),
         stabf = factor(stabf, levels = c("stable", "unstable"), labels = c("Stable", "Instable")))
set.seed(7)
dix_pour_cent_reduits <- dix_pour_cent_reduits[sample(1:nrow(dix_pour_cent_reduits), size = 1000), ]
```

## Description de la base

Chaque observation représentait l'évaluation du système électrique avec 11 variables explicatives et 1 variable à expliquer :

* La variable à expliquer est stabf et nous informait sur la stabilité du réseau électrique : "stable" ou "instable" ;
* Les variables explicatives sont : 
  * $\tau_1$, $\tau_2$, $\tau_3$, $\tau_4$ (électricité produite respectivement à partir du nœud 1, 2, 3 et 4) ; 
  * $P_2$, $P_3$ et  $P_4$ (énergie consommée/produite respectivement sur le 2, 3 et 4ème nœuds). $P_1$ quant à lui, réprésentait la valeur absolue de la somme de $P_2$, $P_3$ et $P_4$. Cette variable étant une combinaison linéaire de 3 autres variables, elle n'apportait aucun information et n'a pas été utilisée ; 
  * $G_1$, $G_2$, $G_3$ et $G_4$ (prix de l'électricité sur les noeuds 1 à 4 à chaque observation).
  
L'ensemble des ces variables permettent de déterminer si le réseau est stable ou non et étaient indépendantes les unes des autres.

Nous avons travaillé sur un échantillon de 1000 observations sélectionnées au hasard parmi les 10000 observations de la base initiale. 361 observations étaient stables et 639 instables. Il n'y avait aucune donnée manquante. L'ordre de grandeur entre les variables était différent, nous avons donc centré et réduit les valeurs.

Nous avons analysé les courbes de densité pour chaque variable en fonction du statut "stable" et "instable". Les courbes étaient superposables pour les variables p contrairement aux autres variabless. Nous pouvions donc faire l'hypothèse que les variables $\tau$ et $\gamma$ étaient plus discriminantes pour classer les observations.

```{r, include = FALSE}
helper <- tibble(noms = names(dix_pour_cent_reduits)[-12],
                 titres = c("&tau;<sub>1</sub>", "&tau;<sub>2</sub>", "&tau;<sub>3</sub>", "&tau;<sub>4</sub>", 
                            "P<sub>2</sub>", "P<sub>3</sub>", "P<sub>4</sub>",
                            "G<sub>1</sub>", "G<sub>2</sub>", "G<sub>3</sub>", "G<sub>4</sub>"))
```

```{r plot-densités, fig.height = 8, fig.width = 10, cache = FALSE, warning = FALSE}
density_plot <- function(var, data, titre){
  ggplot(data = data, aes(x = .data[[var]], colour = stabf, fill = stabf))+
    geom_density(alpha = 0.3) +
    geom_vline(aes(xintercept = mean(.data[[var]])), linetype = "dashed", color = "darkgrey", size = 1) +
    labs(x = titre[["titres"]][match(var, titre[["noms"]])],
         y = "Densité") +  
    scale_fill_discrete(name = "Stabilité du système") +
    scale_color_discrete(name = "Stabilité du système") +
    theme(axis.title.x = element_markdown())
}
legende <- get_legend(density_plot(var = "tau1", data = dix_pour_cent_reduits, titre = helper))
liste_plot <- helper$noms %>% 
  purrr::map(~ density_plot(var = .x, data = dix_pour_cent_reduits, titre = helper) +
               theme(legend.position = "none"))
liste_plot[[12]] <- legende
ggarrange(plotlist = liste_plot, nrow = 3, ncol = 4)
```

L’analyse en composante principale à mis en avant 11 composantes principales qui expliquaient toutes environ la même part de variance (7 à 11%). La représentation des 2 groupes sur les 6 1ers axes principaux, mettait en évidence un nuage de point difficile à individualiser, mais se démarquant quand-même. Globalement, les groupes avaient une forme sphérique et un volume assez équivalent (sur la projection en tout cas).

```{r acp-elec, cache = FALSE, fig.width = 9, fig.height = 7}
ACP_elec <- PCA(dix_pour_cent_reduits, scale.unit = TRUE, graph = FALSE, quali.sup = 12, ncp = 11)
plot_indACP <- function(acp, axes, couleur = "none") {
  plot_axes <- fviz_pca_var(acp, repel = TRUE, axes = axes) +
    labs(title = paste0("Cercle des corrélations des variables aux axes ", paste(axes, collapse = " et "))) +
    theme(plot.title = element_markdown(size = 10))
  plot_ind <- fviz_pca_ind(acp, geom.ind = "point", axes = axes, habillage = couleur) +
    labs(title = paste0("Projection des individus sur les axes ", paste(axes, collapse = " et "), " de l'ACP")) +
    theme(plot.title = element_markdown(size = 10),
          legend.position = "bottom") +
    scale_color_discrete(name = "Stabilité") +
    guides(shape = FALSE,
           color = guide_legend(override.aes = list(shape = c(19, 17))))
  plot_ind + plot_axes
}
plot_indACP(acp = ACP_elec, axes = c(1, 2), couleur = 12)
```

## Mise en oeuvre des modèles

Nous avons mis en oeuvre 3 modèles de classification supervisée : une analyse factorielle discriminante, une analyse des k plus proches voisins et une approche par random forest.

Nous allons essayer de mettre en oeuvre ces classifications et les comparer en estimant les risques de classification pour chacune des 3 techniques étudiées. Pour l'approche par les random forests, le risque sera estimé par les erreurs out-of-the-bag. Pour l'analyse factorielle discriminante et les k plus proches voisins, nous estimerons les risques par cross-validation. Pour ce faire, nous avons aléatoirement découpé notre jeu de données en 10 groupes qui seront les échantillons tests tour à tour afin d'évaluer le risque. Le risque global calculé sera la moyenne des 10 risques obtenus par cette méthode.

De plus, pour l'analyse factorielle discriminante et les k plus proches voisins, nous avons découpé l'échantillon aléatoirement en un échantillon d'apprentissage contenant 70% des observations et un jeu test contenant les 30% restants.

### Analyse factorielle discriminante

```{r crea-ech, cache = FALSE}
set.seed(7)
num_test <- sample(1000, size = 700, replace = FALSE)
ordre_cv <- sample(1000, size = 1000, replace = FALSE)
dix_pour_cent_CV <- cbind(dix_pour_cent_reduits[ordre_cv, ], ech = rep(1:10, each = 100))
dix_pour_cent_app <- dix_pour_cent_reduits[num_test, ]
dix_pour_cent_test <- dix_pour_cent_reduits[-num_test, ]
```

Selon la description de l'ACP, nous avons pris les modèles sphériques. Nous n'étions pas sûr du volume, donc nous avons comparé EII et VII sur le BIC sur l'échantillon d'apprentissage. Pour les 2 cas de figures testés (EDDA = matrices de covariances identiques dans toutes les modalités de la variable à prédire ; et MclustDA = matrices de covariances différentes dans chaque modalité de la variable à prédire), le modèle EII avait été choisi. Pour EDDA, le BIC était plus élevé, et pour MclustDA, les BIC étaient équivalents, mais on minimise le nombre de paramètres avec EII donc nous avons choisi l'approche EII. 

```{r afd, cache = FALSE}
AFD1 <- MclustDA(dix_pour_cent_app[,-12], class = dix_pour_cent_app$stabf, modelType = "EDDA", modelNames = c("EII", "VII"))
BIC_EDDA <- AFD1$models$Stable$BIC
AFD2 <- MclustDA(dix_pour_cent_app[,-12], class = dix_pour_cent_app$stabf, modelType = "MclustDA", modelNames = c("EII", "VII"))
mod_stable <- which.max(AFD2$models$Stable$BIC[, 1])
mod_instable <- which.max(AFD2$models$Instable$BIC[, 1])
BIC_EIIvar <- AFD2$models$Stable$BIC[mod_stable, 1] + AFD2$models$Instable$BIC[mod_instable, 1]
mod_stable2 <- which.max(AFD2$models$Stable$BIC[, 2])
mod_instable2 <- which.max(AFD2$models$Instable$BIC[, 2])
BIC_VIIvar <- AFD2$models$Stable$BIC[mod_stable2, 2] + AFD2$models$Instable$BIC[mod_instable2, 2]
tab_bic_afd <- rbind(BIC_EDDA, cbind(EII = BIC_EIIvar, VII = BIC_VIIvar)) %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  mutate(rowname = c("EDDA", "MclustDA"))
names(tab_bic_afd)[1] <- "Méthode d'estimation de la matrice de covariance"
flextable(tab_bic_afd) %>% 
  bold(i = 1:2, j = "EII") %>% 
  align(align = "center", part = "header") %>% 
  autofit() %>% 
  set_caption("BIC selon l'estimation de la matrice de covariance")
```

Pour retenir ensuite la meilleure méthode concernant l'estimation des matrices de covariance, nous avons estimé le risque avec 2 méthodes : en estimant sur l'échantillon test, et par cross-validation. Sur la méthode de l'échantillon test et celle de cross-validation, la méthode MclustDA donne une meilleure minimisation du risque de classification. C'est donc l'analyse factorielle discriminante avec modèle EII et des matrices de covariances différentes selon les modalités stable et instable qui sera retenu. Le risque de classification pour ce modèle est de 17% en échantillon test et 19.5% en cross-validation. Il est à noter que nous avons aussi comparé avec la fonction `cvMclustDA` du package `mclust` qui donne un risque un peu différent du notre pour la cross-validation de 18%. Mais nous avons préféré utiliser notre méthode pour pouvoir comparer avec les k plus proches voisins.

```{r test-afd, cache = FALSE}
AFD1 <- MclustDA(dix_pour_cent_app[,-12], class = dix_pour_cent_app$stabf, modelType = "EDDA", modelNames = "EII")
AFD2 <- MclustDA(dix_pour_cent_app[,-12], class = dix_pour_cent_app$stabf, modelType = "MclustDA", modelNames = "EII")
vec_test <- c(Méthode = "Sur échantillon test", 
              EDDA = round(summary(AFD1, newdata = dix_pour_cent_test[, -12], newclass = dix_pour_cent_test$stabf)$err.newdata * 100, 1),
              MclustDA = round(summary(AFD2, newdata = dix_pour_cent_test[, -12], newclass = dix_pour_cent_test$stabf)$err.newdata * 100, 1))
```

```{r cv-afd, cache = FALSE}
erreur_AFD <- function(echantillon) {
  test <- dix_pour_cent_CV %>% filter(ech == echantillon)
  train <- dix_pour_cent_CV %>% filter(ech != echantillon)
  AFD1 <- MclustDA(train[,-12], class = train$stabf, modelType = "EDDA", modelNames = "EII")
  err1 <- summary(AFD1, newdata = test[, -12], newclass = test$stabf)$err.newdata
  AFD2 <- MclustDA(train[,-12], class = train$stabf, modelType = "MclustDA", modelNames = "EII")
  err2 <- summary(AFD2, newdata = test[, -12], newclass = test$stabf)$err.newdata
  return(c(edda = err1, MclustDA = err2))
}
cross_val <- map_dfr(1:10, ~ erreur_AFD(.x), .id = "jeu") %>% 
  summarise(across(edda:MclustDA, .fns = ~ round(mean(.x) * 100, 1) %>% as.character)) %>% 
  mutate(Méthode = "Cross-validation", .before = 1) %>% 
  rename(EDDA = "edda")
```

```{r table-valid, cache = FALSE}
bind_rows(vec_test, cross_val) %>% 
  flextable() %>% 
  autofit() %>% 
  bold(i = 1:2, j = 3, part = "body") %>% 
  set_caption("Risque calculé sur le modèle EII selon les 2 méthodes d'estimation de la matrice de covariance en %")
```




Résultats :

2. Analyse de k plus proches voisins

Nous avons réalisé une deuxième analyse en utilisant la méthode des k plus proches voisins avec la même population d’entrainement et de test. Nous avons fait varier le nombre de k voisins sélectionné pour trouver le k optimal puis réalisé un test de validation croisé pour améliorer nos résultats. Le risque était minimal était de 18,3% en utilisant les 11 plus proches voisins.

3. Ramdon Forest

Nous avons évalué la méthode Random Forest. Le risque d’erreur se stabilisait à partir de 4000 arbres. De plus le risque était minimal pour l’utilisation de 3 critères de classifications par arbre. Avec ces paramètres là, risque d’erreur estimé de 11,4%.

Les variables p2, p3, p4 ne semblant pas être discriminantes à la lecture des courbes de densité, nous avons testé cette méthode en en supprimant ces variables. Cela n’améliorait pas notre risque d’erreur de classification. Il était mesuré à 12%.

Conclusion :

Les 3 méthodes utilisées pour réaliser une notre classification supervisée donnait des risques d’erreur différents. La méthode avec le meilleur risque de classification était la Ramdon Forest avec un risque de 14,6%.
