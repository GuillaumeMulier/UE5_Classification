---
title:  "Devoir Classification et Apprentissage session 1"
author: "Benoit Gachet, Guillaume Mulier"
date: "`r format(Sys.Date(), '%d/%m/%Y')`"
output: 
  word_document:
    reference_docx: template_word.docx
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      cache = TRUE)
```

```{r packages, include = FALSE}
library(tidyverse)
library(patchwork)
library(ggtext)
library(FactoMineR)
library(factoextra)
library(dendextend)
library(purrr)
library(scales)
library(mclust)
library(cowplot)
library(ggpubr)
library(FNN)
library(randomForest)
```

```{r helpers, include = FALSE}
ggplot2::theme_set(theme_light() +
                     theme(plot.title = element_markdown(size = 10),
                           plot.subtitle = element_markdown(size = 7),
                           strip.background = element_blank(),
                           strip.text = element_textbox(
                             size = 12, 
                             color = "white", fill = "#7888C0", box.color = "#000066",
                             halign = 0.5, linetype = 1, r = unit(3, "pt"), width = unit(0.75, "npc"),
                             padding = ggplot2::margin(2, 0, 1, 0), margin = ggplot2::margin(3, 3, 3, 3))))
```

# Exercice classification non supervisée

```{r chargement-données, include = FALSE}
les_inconnus_reduits <- read_delim("DonneesNSGpe.txt", delim = "\t") %>% 
  filter(Groupe == 7) %>% 
  select(-Groupe) %>% 
  mutate(across(everything(), .fns = ~ (.x - mean(.x)) / sd(.x)))
les_inconnus_shopenhauer <- les_inconnus_reduits
```

## Introduction

L’objectif de notre devoir était de réaliser une classification non supervisée afin de caractériser l’appartenance des observations de notre base de données à k classes, en définissant le nombre de classes.

Notre base de données comportait un ensemble de 500 observations et 10 variables mesurées pour chacune des observations. Nous n’avions aucune donnée manquante pour l'ensemble de la base. Les ordres de grandeur des variables étaient similaires ; néanmoins, afin d’éviter qu’une variable ne prenne trop d’importance du simple fait de son unité de mesure, nous avons centré et réduit l’ensemble des variables.

Il existe de nombreuses méthodes de classification non supervisées. Nous nous sommes intéressés à 3 méthodes : la classification hiérarchique ascendante, les k-means et les modèles de mélange Gaussiens. Nous les avons ensuite comparé et avons discuté les avantages et les inconvénients de chacune dans notre classification.

## Mise en oeuvre des modèles

### Description du jeu de données

Pour visualiser les données, nous avons mis en oeuvre une analyse en composantes principales. Les 2 1^ères^ composantes principales expliquent la quasi-totalité de l'inertie du jeu de données. Nous utiliserons donc exclusivement ces 2 dimensions pour explorer le jeu de données.

En représentant les projections des individus sur ces 2 composantes, nous pouvons remarquer un nuage de points bien étallé. Et on a l'impression de distinguer 3 groupes de points dans ce nuage. Ces 3 groupes ont l'air d'être de forme sphérique, ce qui guidera nos choix de classifications par la suite.

```{r acp, fig.width = 9, fig.height = 7}
ACP_inc <- PCA(les_inconnus_reduits, scale.unit = TRUE, graph = FALSE)
plot_axes <- fviz_pca_var(ACP_inc, repel = TRUE) +
  labs(title = "Cercle des corrélations des variables aux axes 1 et 2") +
  theme(plot.title = element_markdown(size = 10))
plot_ind <- fviz_pca_ind(ACP_inc, geom.ind = "point") +
  labs(title = "Projection des individus sur les 2 1<sup>ers</sup> axes de l'ACP") +
  theme(plot.title = element_markdown(size = 10))
plot_ind + plot_axes
```

### Classification ascendante hiérarchique

Nous avons dans un premier temps réalisé une classification hiérarchique ascendante et présenté ses résultats sous la forme d'un dendrogramme. 

Etant donné le résultat de l'ACP avec la projection des variables sur un plan qui explique la quasi-totalité de l'inertie, ainsi que le caractère numérique de toutes nos variables prédictrices, nous pouvons bien nous mettre dans la situation d'un espace métrique. De ce fait, nous avons choisi d'utiliser la distance euclidienne entre les différents individus statistiques, et la méthode pour aggréger les individus est la méthode de Ward, adaptée aux clusters sphériques.

Pour déterminer le nombre de clusters, nous avons calculé l'inertie intra-classe en fonction du nombre de clusters. Puis, notre nombre de clusters était le nombre pour lequel il y a une rupture dans la diminution d'inertie intra-classe (stratégie, parfois dénommée ”critère du coude”). Cette classification permet de créer 3 classes d’effectifs semblables puisqu’ils sont respectivement de 156, 168, 176.

```{r cah-dend, message = FALSE, warning = FALSE, fig.width = 10, fig.height = 6}
set.seed(7)
dendro <- les_inconnus_reduits %>% 
  dist(method = "euclidean") %>% 
  hclust(method = "ward.D2") 
dendro_marches <- cbind(x = 1:500, inertie = sort(dendro$height, decreasing = TRUE), color = c(1, 1, 1, rep(2, 497))) %>% 
  as.data.frame() %>% 
  slice(1:20) %>% 
  ggplot(aes(x = x, y = inertie)) +
  geom_step() +
  geom_point(aes(color = as.factor(color)), show.legend = FALSE) +
  geom_vline(xintercept = 3.5, color = "blue") +
  scale_color_manual(values = c("blue", "black")) +
  scale_x_continuous(breaks = 1:20) +
  labs(x = "Nombre de branches",
       y = "Inertie intra-classe",
       title = "Décroissance de l'inertie avec le nombre de branches") +
  theme(panel.grid.minor.x = element_blank())
dendro_arbre <- dendro %>% 
  as.dendrogram() %>%
  set("branches_k_color", k = 3) %>% 
  set("branches_lwd", 0.5) %>%
  set("labels_cex", 0) %>%
  set("leaves_pch", 10) %>% 
  set("leaves_cex", 0.1) %>% 
  as.ggdend() %>% 
  ggplot(theme = theme_light()) +
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        panel.grid.minor.x = element_blank()) +
  labs(y = "Inertie intra-classe",
       x = NULL,
       title = "Classification ascendante hiérarchique selon la distance euclidienne") +
  geom_hline(yintercept = 25, linetype = "dashed", color = "purple3") +
  geom_segment(aes(x = 168, xend = 168, y = 25, yend = 0), linetype = "dashed", color = "purple3") +
  geom_segment(aes(x = 324, xend = 324, y = 25, yend = 0), linetype = "dashed", color = "purple3")
dendro_marches + dendro_arbre
les_inconnus_shopenhauer$class_cah <- cutree(dendro, k = 3)
```
En réprésentant les individus sur les 2 1^ères^ composantes principales, nous pouvons voir les 3 clusters qui sont dans 3 directions du plan représenté.

```{r cah-ind, fig.width = 9, fig.height = 7}
ACP_inc <- PCA(les_inconnus_shopenhauer, scale.unit = TRUE, graph = FALSE, quali.sup = 11)
plot_ind2 <- fviz_pca_ind(ACP_inc, geom.ind = "point", habillage = 11) +
  labs(title = "Projection des individus sur les 2 1<sup>ers</sup> axes de l'ACP",
       subtitle = "Colorés selon les groupes faits par CAH") +
  theme(plot.title = element_markdown(size = 10),
        plot.subtitle = element_text(size = 7),
        legend.position = "none")
plot_ind2 + plot_axes
```

### Les K-means

```{r fct-kmeans, include = FALSE}
replication_kmeans <- function(data, n_rep, nb_clust, result = "modele", nstart) {
  result <- match.arg(result, c("modele", "liste"))
  kmeans_rep <- replicate(n_rep, kmeans(data, centers = nb_clust, nstart = nstart))
  best <- which.min(unlist(kmeans_rep['tot.withinss', ]))
  kmeans <- kmeans_rep[, best]
  if (result == "modele") {
    return(kmeans)
  } else if (result == "liste") {
    return(kmeans_rep)
  }
}
```

Pour sélectionner le nombre de groupes optimal par les méthodes des K-means, nous avons eu recours à 2 méthodes. La 1^ère^ est analogue à celle utilisée pour la classification ascendante hiérarchique : nous avons repéré le saut dans la diminution de l'inertie intra-classe avec le nombre de classes. Nous avons aussi représenté l'indice de Calinski-Harabasz qui doit être maximisé. Ces 2 méthodes nous indiquent un nombre optimal de clusters de 3. Nous réaliserons donc des clusters selon la méthode des K-means avec k = 3.

```{r choix-k, fig.width = 10, fig.height = 6}
set.seed(7)
modeles <- vector(mode = "list", length = 10)
for (i in 1:10) {
  modeles[[i]] <- replication_kmeans(data = les_inconnus_reduits,
                                     nb_clust = i, n_rep = 25, nstart = 25)
}
plot_inertie <- map_dfr(modeles, ~ c(intra = .x[["tot.withinss"]]), .id = "k") %>% 
  bind_cols(color = c(1, 1, 1, rep(2, 7))) %>% 
  ggplot(aes(x = as.numeric(k), y = intra)) +
  geom_point(aes(color = as.factor(color)), show.legend = FALSE) +
  geom_step() +
  geom_vline(xintercept = 3.5, color = "blue") +
  scale_color_manual(values = c("blue", "black")) +
  scale_x_continuous(breaks = 1:10) +
  labs(x = "Nombre de clusters",
       y = "Inertie intra-clusters",
       title = "Décroissance de l'inertie avec le nombre de clusters",
       subtitle = "Sélection de la meilleure classification parmi 25 réplicats à chaque k") +
  theme(panel.grid.minor.x = element_blank())
# Là encore, il y a un saut dans les inerties pour k = 3 environ
plot_calinski <- map_dfr(modeles, ~ c(intra = .x[["tot.withinss"]], inter = .x[["betweenss"]]), .id = "k") %>% 
  mutate(k = as.numeric(k),
         calinski = ifelse(k == 1, 0, ((500 - k) * inter) / ((k - 1) * intra)),
         color = c(2, 2, 1, rep(2, 7))) %>% 
  ggplot(aes(x = k, y = calinski)) +
  geom_point(aes(color = as.factor(color)), show.legend = FALSE) +
  geom_line() +
  geom_vline(xintercept = 3, color = "blue", linetype = "dashed") +
  scale_color_manual(values = c("blue", "black")) +
  scale_x_continuous(breaks = 1:10) +
  labs(x = "Nombre de clusters",
       y = "Indice de Calinski-Harabasz",
       title = "Evolution de l'indice de Calinski-Harabasz en fonction du nombre\nde clusters faits") +
  theme(panel.grid.minor.x = element_blank())
# k = 3 maximise l'indice de calinski
plot_inertie + plot_calinski
```

Pour valider la stabilité de la méthode à l'initialisation aléatoire, nous avons généré 25 réplicats d'une sélection de 3 groupes par les K-means. Les 3 clusters créés ont des effectifs similaires autour de 160-170 dans 24 des réplicats. Mais le 2^ème^ réplicat montre un cluster qui a plus de patients que les 2 autres.

Nous avons donc regardé si les individus étaient répartis différemment selon le réplicat. Pour ce faire, pour les réplicats 2 à 25, nous avons attribué le numéro de cluster du réplicat 1 pour lequel l'effectif dans le réplicat (2 à 25) est majoritaire. Nous avons constaté que dans 70% des individus statistiques, le cluster est constant avec les 25 réplicats. En revanche, pour 30% des individus, on a des clusters différents avec les réplicats. En analysant les séquences de numéros de clusters, on a un déséquilibre pour le réplicat n°2. On peut remarquer que dans ce réplicat, en aggrégeant avec notre méthode, le cluster 1 du réplicat disparaît car il est minoritaire dans tous les clusters du réplicat 1. Ainsi, notre méthode n'est pas totalement satisfaisante dans ce cas de figure précis. Néanmoins, elle nous a permis de montrer la non constance de la classification pour le 2^ème^ réplicat. De plus, nous avons représenté la matrice de similarité des patients qui réprésente les mêmes patients en abscisse et en ordonnée, et les intersections entre 2 individus sont de plus en plus bleues en fonction du nombre de fois où ces 2 individus sont dans le même cluster parmi chaque réplicat. On voit là encore qu'il y a un léger déséquilibre avec un cluster qui n'est pas homogène avec les clusters des autres réplicats et qui est vraisemblablement le cas du réplicat 2.

```{r kmeans-25, fig.width = 10, fig.height = 6}
kmeans_replicats <- replication_kmeans(data = les_inconnus_reduits, nb_clust = 3, n_rep = 25, result = "liste", nstart = 1)
plot_replicats <- kmeans_replicats[7, ] %>% 
  map_dfr(~ c(c1 = .x[1], c2 = .x[2], c3 = .x[3])) %>% 
  bind_cols(rep = 1:25) %>% 
  pivot_longer(cols = -rep) %>% 
  ggplot(aes(rep, value, color = name)) +
  geom_point() +
  geom_line() +
  scale_color_discrete(name = "Clusters :", labels = c("1", "2", "3")) +
  coord_cartesian(ylim = c(80, 320)) +
  labs(x = "Numéro de réplication",
       y = "Effectif",
       title = "Vérification de la taille des clusters en réplicant 25 fois les K-means")
n <- 500
mat <-  matrix(0, nrow = n, ncol = n) # matrice carrée de taille n de 0
clusters <- kmeans_replicats[1, ] %>% 
  map_dfr(~ c(ind = .x)) %>% 
  t() %>% 
  as.data.frame()
clusters <- clusters[order(clusters$V1), ]
# Tri du tableau selon le 1er vecteur de clusters
for (r in seq_len(ncol(clusters))) {
  classe <- clusters[, r]
  for (i in 1:(n-1)) {
    for(j in (i + 1):n) {
      if (classe[i] == classe[j]) mat[i, j] <- mat[i, j] + 1
    }
  }
}
plot_similar <- (mat / ncol(clusters)) %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  pivot_longer(-rowname) %>% 
  mutate(rowname = as.numeric(rowname),
         name = as.numeric(str_remove(name, "^V"))) %>% 
  ggplot(aes(rowname, name, fill = value)) +
  geom_tile() +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_gradient2(low = "white", mid = "green", high = "blue", 
                       midpoint = 0.5, labels = percent_format(),
                       name = NULL) +
  labs(x = NULL,
       y = NULL) +
  annotate(geom = "richtext", x = 100, y = 100, size = 3, hjust = 0, vjust = 1,
           label = "**Visualisation des individus triés selon la <br> 1<sup>ère</sup> réplication de K-means :<br>Pourcentage de fois où les 2 individus sont <br>dans le même cluster sur les 25 réplicats**")
plot_replicats + plot_similar
```

Ainsi, la méthode des K-means apparaît robuste pour classifier notre jeu de données puisque les clusters sont presque constants tout au long des 25 réplicats.

Pour sélectionner notre classification finale, nous avons réalisé là encore 25 réplicats de la classification, puis nous avons sélectionné la classification qui minimisait le plus l'inertie intra-clusters. Nous avons représenté cette classification sur la figure suivante.

```{r plot-kmeanacp, fig.width = 9, fig.height = 7}
kmeans_3 <- replication_kmeans(data = les_inconnus_reduits, nb_clust = 3, n_rep = 25, result = "mode", nstart = 25)
kmeans_3$cluster <- factor(kmeans_3$cluster,
                           levels = c(1, 2, 3), labels = c(2, 1, 3)) %>% 
  as.character() %>% as.numeric()
class(kmeans_3) <- "kmeans"
plot_ind3 <- fviz_cluster(kmeans_3, les_inconnus_reduits, ellipse.type = "convex",
                          geom = "point", ggtheme = theme_light()) +
  labs(title = "Projection des individus sur les 2 1<sup>ers</sup> axes de l'ACP",
       subtitle = "Colorés selon les groupes faits par méthode K-means") +
  theme(plot.title = element_markdown(size = 10),
        plot.subtitle = element_text(size = 7),
        legend.position = "none") +
  scale_x_reverse(labels = function(x) -x)
plot_ind3 + plot_axes 
les_inconnus_shopenhauer$class_kmeans <- kmeans_3$cluster
```

### Modèle de mélanges Gaussiens

A la vue des résultats de l'ACP, nous nous sommes concentré sur les modèles sphériques (EII et VII). Nous avons écarté la méthode EII de notre analyse principale car c'était redondant avec les K-means.

Afin de choisir le nombre de classes, nous nous sommes appuyés sur l’analyse des maximums de vraissemblance, représentés par le score BIC et ICL. Néanmoins, la représentation suivante montre que le BIC et l'ICL n'atteignent pas de valeur maximale pour un nombre de classes de 20 ou moins. Etant donné nos résultats précédents et la représentation de l'ACP, nous nous sommes orienté vers 3 classes. De plus ça a l'air d'être le moment ou il y a une inflexion de l'augmentation du BIC/ICL. Nous avons regardé ce qui se passait pour les modèles non sphériques de matrice de covariance. Pour les modèles non orientés sur les axes, la fonction `Mclust` n'arrivait pas à calculer de classes (dans l'article de Scrucca, il est dit que cela demande une correction de la matrice de covariance), et pour les autres, on n'atteignait pas de maximum ni pour le BIC ni pour l'ICL.

```{r choix-kgaus, fig.width = 10, fig.height = 6}
set.seed(7)
boules_gaus <- purrr::map(1:20, ~ Mclust(les_inconnus_reduits, G = .x, modelNames = "VII"))
boules_gaus %>% 
  map_dfr(~ c(BIC = as.numeric(.x[["bic"]]), ICL = as.numeric(.x[["icl"]])), .id = "k") %>% 
  mutate(k = as.numeric(k)) %>% 
  pivot_longer(-k) %>% 
  ggplot(aes(x = k, y = value)) +
    geom_point() +
    geom_line() +
    labs(x = "Nombre de clusters",
         y = NULL,
         title = "Evolution du BIC et de l'ICL en fonction du nombre de clusters formés en méthode EII") +
    facet_wrap(vars(name), nrow = 1)
```

Nous avons représenté notre classification à 3 classes sur les 2 1^ers^ axes principaux et pouvont constater que les groupes formés sont similaires à ceux obtenus avec les méthodes précédentes avec les ellipses de concentrations selon la loi normale.

```{r fig-gaus3, fig.width = 9, fig.height = 7}
boules_gaus3 <- boules_gaus[[3]]
plot_ind_gaus3 <- fviz_cluster(boules_gaus3, les_inconnus_reduits, ellipse.type = "norm",
                               geom = "point", ggtheme = theme_light()) +
  labs(title = "Projection des individus sur les 2 1<sup>ers</sup> axes de l'ACP",
       subtitle = "Colorés selon les groupes faits par méthode des mélanges Gaussiens",
       caption = "Ellipse de concentration selon la loi normale") +
  theme(plot.title = element_markdown(size = 10),
        plot.subtitle = element_text(size = 7),
        legend.position = "none")
plot_ind_gaus3 <- plot_ind_gaus3 +
  scale_x_reverse(labels = function(x) -x)
plot_ind_gaus3 + plot_axes 
les_inconnus_shopenhauer$class_gaus <- boules_gaus3$classification
```

## Discussion et comparaisons des modèles

L’objectif étant in fine la mise en évidence de la structure sous-jacente des données, une méthode pragmatique pour le choix du nombre de classes est de choisir une partition dont il sera possible d’interpréter les classes.

Le modèle Gaussien minimise la variabilité intra-groupe, mais la sélection du nombre de groupes n'a pas l'air de s'arrêter (sur les critères du BIC et de l'ICL). Nous avons représenté des nombres de clusters croissants sur la figure suivante. En augmentant le nombre de clusters, on remarque que cela ne fait que segmenter les 3 groupes que nous avions définis. Il y a donc peut-être des groupes d'individus à l'intérieur de nos 3 clusters qui se ressemblent et peuvent se regrouper, mais le découpage en 3 groupes apparaît pertinent. La méthode des mélanges Gaussiens n'est peut-être ici pas adaptée pour déterminer le nombre de groupes.

```{r clust-gaus, fig.height = 9, fig.width = 9}
plot_ind_gaussian <- purrr::map(.x = c(3, 5, 7, 9),
                                .f = ~ fviz_cluster(boules_gaus[[.x]], les_inconnus_reduits, ellipse.type = "norm",
                                                    geom = "point", ggtheme = theme_light()) +
                                  labs(title = NULL,
                                       subtitle = paste0("Colorés selon les groupes faits par méthode des mélanges Gaussiens\nk = ", .x)) +
                                  theme(plot.title = element_markdown(size = 10),
                                        plot.subtitle = element_text(size = 7),
                                        legend.position = "none") +
                                  scale_x_reverse(labels = function(x) -x))
wrap_plots(plot_ind_gaussian) + plot_annotation(title = "Modèles de mélanges Gaussiens pour K croissants")
```

Les groupes formés par les 3 méthodes sont très similaires, et on voit que peu d'individus sont dans des clusters différents entre les méthodes. La différence se fait surtout sur les individus à la frontière entre les groupes 1 et 3 (aux coordonnées x = 1 et y = -0.5 environ sur les 2 1^ers^ axes principaux). Nous pensons donc avoir une classification assez proche de la réalité. La méthode des mélanges Gaussiens donne les 3 mêmes groupes environ, mais a plus de mal pour déterminer le nombre de groupes.

```{r comp-nonsup, fig.width = 9, fig.height = 9}
plot_diff <- les_inconnus_shopenhauer %>%
  select(starts_with("class")) %>%
  arrange(class_cah) %>%
  rownames_to_column() %>%
  mutate(rowname = as.numeric(rowname)) %>%
  pivot_longer(-rowname) %>%
  mutate(name = fct_recode(name,
                           "CAH" = "class_cah",
                           "Mélanges de\nGaussiennes" = "class_gaus",
                           "K-means" = "class_kmeans")) %>%
  ggplot(aes(x = as.factor(name), y = rowname, fill = as.factor(value))) +
  scale_x_discrete(name = "Techniques de classification non supervisée", expand = c(0, 0)) +
  scale_y_continuous(name = "Nombre d'individus", expand = c(0, 0)) +
  scale_fill_discrete(name = "Cluster n°") +
  geom_tile() +
  purrr::map(c(1.5, 2.5), ~ geom_vline(aes(xintercept = .x), size = 1, color = "black")) +
  labs(title = "Clusters d'appartenance pour chaque technique")
(plot_ind2 + plot_ind3) / (plot_ind_gaus3 + plot_diff)
```

